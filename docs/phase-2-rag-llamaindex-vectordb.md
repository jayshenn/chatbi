# é˜¶æ®µ 2ï¼šRAG from scratch + LlamaIndex + å‘é‡æ•°æ®åº“

> é¢„è®¡æ—¶é—´ï¼š3 å‘¨

## å­¦ä¹ ç›®æ ‡

- ä»é›¶å®ç° RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æµç¨‹
- æŒæ¡æ–‡æ¡£è§£æã€åˆ‡åˆ†ã€å‘é‡åŒ–çš„æ ¸å¿ƒæ¦‚å¿µ
- ä½¿ç”¨å‘é‡æ•°æ®åº“ï¼ˆChroma/Qdrantï¼‰å­˜å‚¨å’Œæ£€ç´¢
- ä½¿ç”¨ LlamaIndex æ¡†æ¶é‡æ„ RAG

## å‰ç½®æ¡ä»¶

- å®Œæˆ [é˜¶æ®µ 1](phase-1-python-prompt-structured-output.md)
- å‡†å¤‡ä¸€äº›ä¸šåŠ¡æ–‡æ¡£ï¼ˆæŒ‡æ ‡å£å¾„ã€è¡¨ç»“æ„è¯´æ˜ç­‰ï¼‰

---

## Part 1: æ‰‹å†™ RAGï¼ˆä¸ç”¨æ¡†æ¶ï¼‰

### Step 1: å‡†å¤‡æ–‡æ¡£æ•°æ®

#### 1.1 åˆ›å»ºç¤ºä¾‹æ–‡æ¡£ç›®å½•

```bash
mkdir -p data/docs
```

#### 1.2 åˆ›å»ºç¤ºä¾‹æ–‡æ¡£

åˆ›å»º `data/docs/metrics.md`ï¼š

```markdown
# æ ¸å¿ƒä¸šåŠ¡æŒ‡æ ‡è¯´æ˜

## GMVï¼ˆGross Merchandise Volumeï¼‰

**å®šä¹‰**ï¼šæˆäº¤æ€»é¢ï¼ŒæŒ‡ä¸€å®šæ—¶é—´å†…çš„æˆäº¤é‡‘é¢æ€»å’Œã€‚

**è®¡ç®—å…¬å¼**ï¼š
```
GMV = SUM(è®¢å•é‡‘é¢)
WHERE è®¢å•çŠ¶æ€ IN ('å·²æ”¯ä»˜', 'å·²å‘è´§', 'å·²å®Œæˆ')
```

**æ³¨æ„äº‹é¡¹**ï¼š
- ä¸åŒ…å«å–æ¶ˆå’Œé€€æ¬¾è®¢å•
- åŒ…å«è¿è´¹å’Œä¼˜æƒ å‰çš„åŸä»·

## DAUï¼ˆDaily Active Usersï¼‰

**å®šä¹‰**ï¼šæ—¥æ´»è·ƒç”¨æˆ·æ•°ï¼Œå½“å¤©æœ‰ç™»å½•æˆ–è®¿é—®è¡Œä¸ºçš„ç‹¬ç«‹ç”¨æˆ·æ•°ã€‚

**è®¡ç®—å…¬å¼**ï¼š
```
DAU = COUNT(DISTINCT user_id)
WHERE event_date = ç›®æ ‡æ—¥æœŸ
AND event_type IN ('login', 'page_view', 'click')
```

## ç”¨æˆ·ç•™å­˜ç‡

**å®šä¹‰**ï¼šåœ¨æŸä¸ªæ—¶é—´ç‚¹æ–°å¢çš„ç”¨æˆ·ä¸­ï¼Œç»è¿‡ä¸€æ®µæ—¶é—´åä»ç„¶æ´»è·ƒçš„ç”¨æˆ·æ¯”ä¾‹ã€‚

**æ¬¡æ—¥ç•™å­˜**ï¼š
```
æ¬¡æ—¥ç•™å­˜ç‡ = æ¬¡æ—¥æ´»è·ƒçš„æ–°ç”¨æˆ·æ•° / å½“æ—¥æ–°å¢ç”¨æˆ·æ•° * 100%
```

**7æ—¥ç•™å­˜**ï¼š
```
7æ—¥ç•™å­˜ç‡ = ç¬¬7å¤©æ´»è·ƒçš„æ–°ç”¨æˆ·æ•° / å½“æ—¥æ–°å¢ç”¨æˆ·æ•° * 100%
```
```

åˆ›å»º `data/docs/tables.md`ï¼š

```markdown
# æ•°æ®è¡¨ç»“æ„è¯´æ˜

## ç”¨æˆ·è¡¨ (users)

| å­—æ®µå | ç±»å‹ | è¯´æ˜ |
|--------|------|------|
| id | BIGINT | ç”¨æˆ·IDï¼Œä¸»é”® |
| name | VARCHAR(100) | ç”¨æˆ·å |
| email | VARCHAR(200) | é‚®ç®± |
| phone | VARCHAR(20) | æ‰‹æœºå· |
| created_at | TIMESTAMP | æ³¨å†Œæ—¶é—´ |
| status | VARCHAR(20) | çŠ¶æ€ï¼šactive/inactive/banned |

## è®¢å•è¡¨ (orders)

| å­—æ®µå | ç±»å‹ | è¯´æ˜ |
|--------|------|------|
| id | BIGINT | è®¢å•IDï¼Œä¸»é”® |
| user_id | BIGINT | ç”¨æˆ·IDï¼Œå¤–é”®å…³è” users.id |
| amount | DECIMAL(10,2) | è®¢å•é‡‘é¢ |
| status | VARCHAR(20) | è®¢å•çŠ¶æ€ï¼špending/paid/shipped/completed/cancelled |
| created_at | TIMESTAMP | ä¸‹å•æ—¶é—´ |
| paid_at | TIMESTAMP | æ”¯ä»˜æ—¶é—´ |

## ç”¨æˆ·è¡Œä¸ºè¡¨ (user_events)

| å­—æ®µå | ç±»å‹ | è¯´æ˜ |
|--------|------|------|
| id | BIGINT | äº‹ä»¶ID |
| user_id | BIGINT | ç”¨æˆ·ID |
| event_type | VARCHAR(50) | äº‹ä»¶ç±»å‹ï¼šlogin/page_view/click/purchase |
| event_date | DATE | äº‹ä»¶æ—¥æœŸ |
| event_time | TIMESTAMP | äº‹ä»¶æ—¶é—´ |
| page_url | VARCHAR(500) | é¡µé¢URL |
```

### Step 2: æ–‡æ¡£è§£æä¸åˆ‡åˆ†

#### 2.1 æ·»åŠ ä¾èµ–

æ›´æ–° `pyproject.toml`ï¼š

```toml
dependencies = [
    # ... å·²æœ‰ä¾èµ–
    "tiktoken>=0.7.0",      # Token è®¡æ•°
    "numpy>=1.26.0",        # å‘é‡è®¡ç®—
    "chromadb>=0.5.0",      # å‘é‡æ•°æ®åº“
]
```

```bash
uv pip install -e ".[dev]"
```

#### 2.2 åˆ›å»ºæ–‡æ¡£å¤„ç†æ¨¡å—

åˆ›å»º `src/chatbi/rag/__init__.py`ï¼š

```python
"""RAG æ¨¡å—"""
```

åˆ›å»º `src/chatbi/rag/document.py`ï¼š

```python
"""æ–‡æ¡£æ•°æ®ç»“æ„"""

from dataclasses import dataclass, field
from typing import Any
import hashlib


@dataclass
class Document:
    """æ–‡æ¡£"""

    content: str
    metadata: dict[str, Any] = field(default_factory=dict)

    @property
    def doc_id(self) -> str:
        """ç”Ÿæˆæ–‡æ¡£å”¯ä¸€ ID"""
        source = self.metadata.get("source", "")
        return hashlib.md5(f"{source}:{self.content[:100]}".encode()).hexdigest()


@dataclass
class Chunk:
    """æ–‡æ¡£ç‰‡æ®µ"""

    content: str
    metadata: dict[str, Any] = field(default_factory=dict)
    embedding: list[float] | None = None

    @property
    def chunk_id(self) -> str:
        """ç”Ÿæˆç‰‡æ®µå”¯ä¸€ ID"""
        source = self.metadata.get("source", "")
        index = self.metadata.get("chunk_index", 0)
        return hashlib.md5(f"{source}:{index}:{self.content[:50]}".encode()).hexdigest()
```

#### 2.3 åˆ›å»ºæ–‡æ¡£åŠ è½½å™¨

åˆ›å»º `src/chatbi/rag/loader.py`ï¼š

```python
"""æ–‡æ¡£åŠ è½½å™¨"""

from pathlib import Path

from chatbi.rag.document import Document


def load_markdown(file_path: str | Path) -> Document:
    """åŠ è½½ Markdown æ–‡ä»¶"""
    path = Path(file_path)
    with open(path, "r", encoding="utf-8") as f:
        content = f.read()

    return Document(
        content=content,
        metadata={
            "source": str(path),
            "filename": path.name,
            "filetype": "markdown",
        },
    )


def load_text(file_path: str | Path) -> Document:
    """åŠ è½½çº¯æ–‡æœ¬æ–‡ä»¶"""
    path = Path(file_path)
    with open(path, "r", encoding="utf-8") as f:
        content = f.read()

    return Document(
        content=content,
        metadata={
            "source": str(path),
            "filename": path.name,
            "filetype": "text",
        },
    )


def load_directory(dir_path: str | Path, extensions: list[str] | None = None) -> list[Document]:
    """
    åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰æ–‡æ¡£

    Args:
        dir_path: ç›®å½•è·¯å¾„
        extensions: æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼Œé»˜è®¤ [".md", ".txt"]
    """
    if extensions is None:
        extensions = [".md", ".txt"]

    docs = []
    path = Path(dir_path)

    for ext in extensions:
        for file_path in path.glob(f"**/*{ext}"):
            if ext == ".md":
                docs.append(load_markdown(file_path))
            else:
                docs.append(load_text(file_path))

    return docs
```

#### 2.4 åˆ›å»ºæ–‡æœ¬åˆ‡åˆ†å™¨

åˆ›å»º `src/chatbi/rag/splitter.py`ï¼š

```python
"""æ–‡æœ¬åˆ‡åˆ†å™¨"""

import re
from typing import Callable

import tiktoken

from chatbi.rag.document import Chunk, Document


def count_tokens(text: str, model: str = "gpt-4") -> int:
    """è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡"""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))


class TextSplitter:
    """æ–‡æœ¬åˆ‡åˆ†å™¨"""

    def __init__(
        self,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        length_function: Callable[[str], int] = count_tokens,
    ):
        """
        Args:
            chunk_size: æ¯å—æœ€å¤§é•¿åº¦ï¼ˆtoken æ•°æˆ–å­—ç¬¦æ•°ï¼‰
            chunk_overlap: å—ä¹‹é—´çš„é‡å é•¿åº¦
            length_function: è®¡ç®—æ–‡æœ¬é•¿åº¦çš„å‡½æ•°
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.length_function = length_function

    def split_text(self, text: str) -> list[str]:
        """å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºå¤šä¸ªç‰‡æ®µ"""
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r"\n\n+", text)

        chunks = []
        current_chunk = []
        current_length = 0

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            para_length = self.length_function(para)

            # å¦‚æœå•ä¸ªæ®µè½è¶…è¿‡ chunk_sizeï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ‡åˆ†
            if para_length > self.chunk_size:
                # å…ˆä¿å­˜å½“å‰å—
                if current_chunk:
                    chunks.append("\n\n".join(current_chunk))
                    current_chunk = []
                    current_length = 0

                # æŒ‰å¥å­åˆ‡åˆ†å¤§æ®µè½
                sentences = re.split(r"(?<=[ã€‚ï¼ï¼Ÿ.!?])\s*", para)
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    sent_length = self.length_function(sentence)
                    if current_length + sent_length <= self.chunk_size:
                        current_chunk.append(sentence)
                        current_length += sent_length
                    else:
                        if current_chunk:
                            chunks.append(" ".join(current_chunk))
                        current_chunk = [sentence]
                        current_length = sent_length
            else:
                if current_length + para_length <= self.chunk_size:
                    current_chunk.append(para)
                    current_length += para_length
                else:
                    # ä¿å­˜å½“å‰å—ï¼Œå¼€å§‹æ–°å—
                    if current_chunk:
                        chunks.append("\n\n".join(current_chunk))

                    # æ·»åŠ é‡å 
                    if self.chunk_overlap > 0 and chunks:
                        overlap_text = self._get_overlap(chunks[-1])
                        current_chunk = [overlap_text, para] if overlap_text else [para]
                        current_length = self.length_function("\n\n".join(current_chunk))
                    else:
                        current_chunk = [para]
                        current_length = para_length

        # å¤„ç†æœ€åä¸€å—
        if current_chunk:
            chunks.append("\n\n".join(current_chunk))

        return chunks

    def _get_overlap(self, text: str) -> str:
        """è·å–é‡å éƒ¨åˆ†"""
        if self.chunk_overlap <= 0:
            return ""

        # ä»æœ«å°¾æˆªå–å¤§çº¦ overlap é•¿åº¦çš„æ–‡æœ¬
        words = text.split()
        overlap_words = []
        current_length = 0

        for word in reversed(words):
            word_length = self.length_function(word)
            if current_length + word_length <= self.chunk_overlap:
                overlap_words.insert(0, word)
                current_length += word_length
            else:
                break

        return " ".join(overlap_words)

    def split_document(self, doc: Document) -> list[Chunk]:
        """å°†æ–‡æ¡£åˆ‡åˆ†ä¸ºå¤šä¸ªç‰‡æ®µ"""
        texts = self.split_text(doc.content)

        chunks = []
        for i, text in enumerate(texts):
            chunk = Chunk(
                content=text,
                metadata={
                    **doc.metadata,
                    "chunk_index": i,
                    "total_chunks": len(texts),
                },
            )
            chunks.append(chunk)

        return chunks


class MarkdownSplitter(TextSplitter):
    """Markdown æ„ŸçŸ¥çš„åˆ‡åˆ†å™¨"""

    def split_text(self, text: str) -> list[str]:
        """æŒ‰æ ‡é¢˜åˆ‡åˆ† Markdown"""
        # æŒ‰ä¸€çº§å’ŒäºŒçº§æ ‡é¢˜åˆ†å‰²
        sections = re.split(r"\n(?=##?\s)", text)

        chunks = []
        for section in sections:
            section = section.strip()
            if not section:
                continue

            section_length = self.length_function(section)
            if section_length <= self.chunk_size:
                chunks.append(section)
            else:
                # ç« èŠ‚å¤ªé•¿ï¼Œè¿›ä¸€æ­¥åˆ‡åˆ†
                sub_chunks = super().split_text(section)
                chunks.extend(sub_chunks)

        return chunks
```

### Step 3: Embedding + å‘é‡æ£€ç´¢

#### 3.1 åˆ›å»º Embedding æ¨¡å—

åˆ›å»º `src/chatbi/rag/embedding.py`ï¼š

```python
"""Embedding å‘é‡åŒ–"""

import numpy as np
from openai import OpenAI

from chatbi.config import get_settings


def get_embedding_client() -> OpenAI:
    """è·å– Embedding å®¢æˆ·ç«¯"""
    settings = get_settings()
    return OpenAI(
        api_key=settings.openai_api_key,
        base_url=settings.openai_base_url,
    )


def get_embedding(text: str, model: str = "text-embedding-3-small") -> list[float]:
    """
    è·å–æ–‡æœ¬çš„ embedding å‘é‡

    Args:
        text: è¾“å…¥æ–‡æœ¬
        model: embedding æ¨¡å‹åç§°
    """
    client = get_embedding_client()
    response = client.embeddings.create(input=text, model=model)
    return response.data[0].embedding


def get_embeddings(texts: list[str], model: str = "text-embedding-3-small") -> list[list[float]]:
    """
    æ‰¹é‡è·å–æ–‡æœ¬çš„ embedding å‘é‡

    Args:
        texts: è¾“å…¥æ–‡æœ¬åˆ—è¡¨
        model: embedding æ¨¡å‹åç§°
    """
    client = get_embedding_client()
    response = client.embeddings.create(input=texts, model=model)
    return [item.embedding for item in response.data]


def cosine_similarity(vec1: list[float], vec2: list[float]) -> float:
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    a = np.array(vec1)
    b = np.array(vec2)
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))


class SimpleVectorStore:
    """ç®€å•çš„å†…å­˜å‘é‡å­˜å‚¨ï¼ˆä»…ç”¨äºå­¦ä¹ ç†è§£åŸç†ï¼‰"""

    def __init__(self):
        self.chunks: list[tuple[str, list[float], dict]] = []  # (content, embedding, metadata)

    def add(self, content: str, embedding: list[float], metadata: dict | None = None):
        """æ·»åŠ å‘é‡"""
        self.chunks.append((content, embedding, metadata or {}))

    def search(self, query_embedding: list[float], top_k: int = 5) -> list[dict]:
        """
        ç›¸ä¼¼åº¦æ£€ç´¢

        Returns:
            [{"content": str, "score": float, "metadata": dict}, ...]
        """
        results = []
        for content, embedding, metadata in self.chunks:
            score = cosine_similarity(query_embedding, embedding)
            results.append({
                "content": content,
                "score": score,
                "metadata": metadata,
            })

        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:top_k]
```

#### 3.2 åˆ›å»º Chroma å‘é‡å­˜å‚¨

åˆ›å»º `src/chatbi/rag/vectorstore.py`ï¼š

```python
"""å‘é‡æ•°æ®åº“å°è£…"""

from pathlib import Path
from typing import Any

import chromadb
from chromadb.config import Settings

from chatbi.rag.document import Chunk
from chatbi.rag.embedding import get_embedding, get_embeddings


class ChromaVectorStore:
    """Chroma å‘é‡æ•°æ®åº“å°è£…"""

    def __init__(
        self,
        collection_name: str = "chatbi",
        persist_directory: str | None = None,
    ):
        """
        Args:
            collection_name: é›†åˆåç§°
            persist_directory: æŒä¹…åŒ–ç›®å½•ï¼ŒNone åˆ™ä½¿ç”¨å†…å­˜æ¨¡å¼
        """
        if persist_directory:
            self.client = chromadb.PersistentClient(path=persist_directory)
        else:
            self.client = chromadb.Client()

        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"},  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
        )

    def add_chunks(self, chunks: list[Chunk], batch_size: int = 100):
        """
        æ·»åŠ æ–‡æ¡£ç‰‡æ®µåˆ°å‘é‡åº“

        Args:
            chunks: æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i : i + batch_size]

            ids = [chunk.chunk_id for chunk in batch]
            documents = [chunk.content for chunk in batch]
            metadatas = [chunk.metadata for chunk in batch]

            # æ‰¹é‡è·å– embeddings
            embeddings = get_embeddings(documents)

            self.collection.add(
                ids=ids,
                documents=documents,
                embeddings=embeddings,
                metadatas=metadatas,
            )

    def search(
        self,
        query: str,
        top_k: int = 5,
        where: dict | None = None,
    ) -> list[dict]:
        """
        ç›¸ä¼¼åº¦æ£€ç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›æ•°é‡
            where: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶

        Returns:
            [{"content": str, "score": float, "metadata": dict}, ...]
        """
        query_embedding = get_embedding(query)

        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            where=where,
            include=["documents", "metadatas", "distances"],
        )

        items = []
        if results["documents"] and results["documents"][0]:
            for i, doc in enumerate(results["documents"][0]):
                items.append({
                    "content": doc,
                    "score": 1 - results["distances"][0][i],  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                    "metadata": results["metadatas"][0][i] if results["metadatas"] else {},
                })

        return items

    def delete_collection(self):
        """åˆ é™¤é›†åˆ"""
        self.client.delete_collection(self.collection.name)

    def count(self) -> int:
        """è·å–æ–‡æ¡£æ•°é‡"""
        return self.collection.count()
```

### Step 4: RAG æŸ¥è¯¢æµç¨‹

#### 4.1 åˆ›å»º RAG æŸ¥è¯¢å¼•æ“

åˆ›å»º `src/chatbi/rag/query.py`ï¼š

```python
"""RAG æŸ¥è¯¢å¼•æ“"""

from dataclasses import dataclass

from chatbi.llm import chat_completion
from chatbi.rag.vectorstore import ChromaVectorStore


@dataclass
class RAGResponse:
    """RAG å“åº”"""

    answer: str
    sources: list[dict]  # [{"content": str, "score": float, "metadata": dict}]


RAG_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æåŠ©æ‰‹ï¼Œè´Ÿè´£å›ç­”ç”¨æˆ·å…³äºæ•°æ®æŒ‡æ ‡ã€è¡¨ç»“æ„ã€æ•°ä»“è®¾è®¡ç­‰é—®é¢˜ã€‚

## å›ç­”è¦æ±‚
1. åªåŸºäºæä¾›çš„å‚è€ƒæ–‡æ¡£å›ç­”é—®é¢˜
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·
3. å›ç­”è¦å‡†ç¡®ã€ç®€æ´
4. å¦‚æœå¼•ç”¨äº†æ–‡æ¡£å†…å®¹ï¼Œè¯·æ ‡æ³¨æ¥æº

## å‚è€ƒæ–‡æ¡£
{context}
"""

RAG_USER_PROMPT = """è¯·æ ¹æ®å‚è€ƒæ–‡æ¡£å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

é—®é¢˜ï¼š{question}
"""


class RAGQueryEngine:
    """RAG æŸ¥è¯¢å¼•æ“"""

    def __init__(
        self,
        vectorstore: ChromaVectorStore,
        top_k: int = 3,
    ):
        self.vectorstore = vectorstore
        self.top_k = top_k

    def query(self, question: str) -> RAGResponse:
        """
        æ‰§è¡Œ RAG æŸ¥è¯¢

        Args:
            question: ç”¨æˆ·é—®é¢˜

        Returns:
            RAG å“åº”ï¼ŒåŒ…å«ç­”æ¡ˆå’Œå¼•ç”¨æ¥æº
        """
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved = self.vectorstore.search(question, top_k=self.top_k)

        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, item in enumerate(retrieved, 1):
            source = item["metadata"].get("filename", "æœªçŸ¥æ¥æº")
            context_parts.append(f"[æ–‡æ¡£{i}] æ¥æº: {source}\n{item['content']}")

        context = "\n\n---\n\n".join(context_parts)

        # 3. æ„å»º Prompt
        system_prompt = RAG_SYSTEM_PROMPT.format(context=context)
        user_prompt = RAG_USER_PROMPT.format(question=question)

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        # 4. è°ƒç”¨ LLM
        answer = chat_completion(messages, temperature=0.3)

        return RAGResponse(answer=answer, sources=retrieved)
```

#### 4.2 åˆ›å»ºç´¢å¼•æ„å»ºè„šæœ¬

åˆ›å»º `src/chatbi/rag/build_index.py`ï¼š

```python
"""æ„å»º RAG ç´¢å¼•"""

import argparse
from pathlib import Path

from chatbi.rag.loader import load_directory
from chatbi.rag.splitter import MarkdownSplitter
from chatbi.rag.vectorstore import ChromaVectorStore


def build_index(
    docs_dir: str,
    persist_dir: str = "./data/vectordb",
    collection_name: str = "chatbi",
    chunk_size: int = 500,
    chunk_overlap: int = 50,
):
    """
    æ„å»º RAG ç´¢å¼•

    Args:
        docs_dir: æ–‡æ¡£ç›®å½•
        persist_dir: å‘é‡åº“æŒä¹…åŒ–ç›®å½•
        collection_name: é›†åˆåç§°
        chunk_size: åˆ‡åˆ†å—å¤§å°
        chunk_overlap: å—é‡å å¤§å°
    """
    print(f"ğŸ“‚ åŠ è½½æ–‡æ¡£: {docs_dir}")
    docs = load_directory(docs_dir)
    print(f"   æ‰¾åˆ° {len(docs)} ä¸ªæ–‡æ¡£")

    print("âœ‚ï¸  åˆ‡åˆ†æ–‡æ¡£...")
    splitter = MarkdownSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    all_chunks = []
    for doc in docs:
        chunks = splitter.split_document(doc)
        all_chunks.extend(chunks)
        print(f"   {doc.metadata['filename']}: {len(chunks)} ä¸ªç‰‡æ®µ")

    print(f"ğŸ“Š æ€»è®¡ {len(all_chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")

    print(f"ğŸ”„ å‘é‡åŒ–å¹¶å­˜å…¥æ•°æ®åº“: {persist_dir}")
    vectorstore = ChromaVectorStore(
        collection_name=collection_name,
        persist_directory=persist_dir,
    )
    vectorstore.add_chunks(all_chunks)

    print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {vectorstore.count()} æ¡è®°å½•")


def main():
    parser = argparse.ArgumentParser(description="æ„å»º RAG ç´¢å¼•")
    parser.add_argument("--docs", type=str, default="./data/docs", help="æ–‡æ¡£ç›®å½•")
    parser.add_argument("--persist", type=str, default="./data/vectordb", help="å‘é‡åº“ç›®å½•")
    parser.add_argument("--collection", type=str, default="chatbi", help="é›†åˆåç§°")
    parser.add_argument("--chunk-size", type=int, default=500, help="åˆ‡åˆ†å—å¤§å°")
    parser.add_argument("--overlap", type=int, default=50, help="å—é‡å å¤§å°")

    args = parser.parse_args()

    build_index(
        docs_dir=args.docs,
        persist_dir=args.persist,
        collection_name=args.collection,
        chunk_size=args.chunk_size,
        chunk_overlap=args.overlap,
    )


if __name__ == "__main__":
    main()
```

#### 4.3 æ·»åŠ  API æ¥å£

æ›´æ–° `src/chatbi/main.py`ï¼Œæ·»åŠ  RAG æ¥å£ï¼š

```python
from chatbi.rag.vectorstore import ChromaVectorStore
from chatbi.rag.query import RAGQueryEngine, RAGResponse


# åˆå§‹åŒ– RAG å¼•æ“ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
_rag_engine: RAGQueryEngine | None = None


def get_rag_engine() -> RAGQueryEngine:
    global _rag_engine
    if _rag_engine is None:
        vectorstore = ChromaVectorStore(
            collection_name="chatbi",
            persist_directory="./data/vectordb",
        )
        _rag_engine = RAGQueryEngine(vectorstore)
    return _rag_engine


class AskRAGRequest(BaseModel):
    """RAG æŸ¥è¯¢è¯·æ±‚"""
    question: str


class SourceItem(BaseModel):
    """å¼•ç”¨æ¥æº"""
    content: str
    score: float
    source: str


class AskRAGResponse(BaseModel):
    """RAG æŸ¥è¯¢å“åº”"""
    answer: str
    sources: list[SourceItem]


@app.post("/ask_rag_raw", response_model=AskRAGResponse)
async def ask_rag_raw(request: AskRAGRequest):
    """æ‰‹å†™ RAG æŸ¥è¯¢æ¥å£"""
    engine = get_rag_engine()
    result = engine.query(request.question)

    sources = [
        SourceItem(
            content=s["content"][:200] + "..." if len(s["content"]) > 200 else s["content"],
            score=s["score"],
            source=s["metadata"].get("filename", "æœªçŸ¥"),
        )
        for s in result.sources
    ]

    return AskRAGResponse(answer=result.answer, sources=sources)
```

### Step 5: æµ‹è¯•æ‰‹å†™ RAG

```bash
# 1. æ„å»ºç´¢å¼•
python -m chatbi.rag.build_index --docs ./data/docs

# 2. å¯åŠ¨æœåŠ¡
uvicorn chatbi.main:app --reload

# 3. æµ‹è¯•æŸ¥è¯¢
curl -X POST http://localhost:8000/ask_rag_raw \
  -H "Content-Type: application/json" \
  -d '{"question": "GMV çš„è®¡ç®—å…¬å¼æ˜¯ä»€ä¹ˆï¼Ÿ"}'

curl -X POST http://localhost:8000/ask_rag_raw \
  -H "Content-Type: application/json" \
  -d '{"question": "è®¢å•è¡¨æœ‰å“ªäº›å­—æ®µï¼Ÿ"}'

curl -X POST http://localhost:8000/ask_rag_raw \
  -H "Content-Type: application/json" \
  -d '{"question": "å¦‚ä½•è®¡ç®—ç”¨æˆ·æ¬¡æ—¥ç•™å­˜ç‡ï¼Ÿ"}'
```

---

## Part 2: ä½¿ç”¨ LlamaIndex é‡å†™ RAG

### Step 6: å®‰è£… LlamaIndex

```bash
# æ›´æ–° pyproject.toml
# dependencies = [
#     ...
#     "llama-index>=0.11.0",
#     "llama-index-vector-stores-chroma>=0.2.0",
#     "llama-index-embeddings-openai>=0.2.0",
# ]

uv pip install -e ".[dev]"
```

### Step 7: LlamaIndex RAG å®ç°

åˆ›å»º `src/chatbi/rag_llamaindex/__init__.py`ï¼š

```python
"""LlamaIndex RAG æ¨¡å—"""
```

åˆ›å»º `src/chatbi/rag_llamaindex/index.py`ï¼š

```python
"""LlamaIndex ç´¢å¼•ç®¡ç†"""

from pathlib import Path

from llama_index.core import (
    Settings,
    SimpleDirectoryReader,
    StorageContext,
    VectorStoreIndex,
    load_index_from_storage,
)
from llama_index.core.node_parser import MarkdownNodeParser, SentenceSplitter
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

from chatbi.config import get_settings


def setup_llama_index():
    """é…ç½® LlamaIndex å…¨å±€è®¾ç½®"""
    settings = get_settings()

    Settings.llm = OpenAI(
        model=settings.model_name,
        api_key=settings.openai_api_key,
        api_base=settings.openai_base_url,
        temperature=0.3,
    )

    Settings.embed_model = OpenAIEmbedding(
        model="text-embedding-3-small",
        api_key=settings.openai_api_key,
        api_base=settings.openai_base_url,
    )


def build_llamaindex(
    docs_dir: str,
    persist_dir: str = "./data/llamaindex_db",
    collection_name: str = "chatbi_llamaindex",
) -> VectorStoreIndex:
    """
    ä½¿ç”¨ LlamaIndex æ„å»ºç´¢å¼•

    Args:
        docs_dir: æ–‡æ¡£ç›®å½•
        persist_dir: æŒä¹…åŒ–ç›®å½•
        collection_name: é›†åˆåç§°
    """
    setup_llama_index()

    # åŠ è½½æ–‡æ¡£
    documents = SimpleDirectoryReader(docs_dir).load_data()
    print(f"ğŸ“‚ åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

    # é…ç½®å‘é‡å­˜å‚¨
    db = chromadb.PersistentClient(path=persist_dir)
    chroma_collection = db.get_or_create_collection(collection_name)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)

    # é…ç½®èŠ‚ç‚¹è§£æå™¨
    node_parser = MarkdownNodeParser()

    # æ„å»ºç´¢å¼•
    index = VectorStoreIndex.from_documents(
        documents,
        storage_context=storage_context,
        transformations=[node_parser],
        show_progress=True,
    )

    print("âœ… LlamaIndex ç´¢å¼•æ„å»ºå®Œæˆ")
    return index


def load_llamaindex(
    persist_dir: str = "./data/llamaindex_db",
    collection_name: str = "chatbi_llamaindex",
) -> VectorStoreIndex:
    """åŠ è½½å·²æœ‰çš„ LlamaIndex ç´¢å¼•"""
    setup_llama_index()

    db = chromadb.PersistentClient(path=persist_dir)
    chroma_collection = db.get_or_create_collection(collection_name)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

    index = VectorStoreIndex.from_vector_store(vector_store)
    return index
```

åˆ›å»º `src/chatbi/rag_llamaindex/query.py`ï¼š

```python
"""LlamaIndex æŸ¥è¯¢å¼•æ“"""

from dataclasses import dataclass

from llama_index.core import VectorStoreIndex
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.postprocessor import SimilarityPostprocessor


@dataclass
class LlamaIndexResponse:
    """LlamaIndex å“åº”"""

    answer: str
    sources: list[dict]


QUERY_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æåŠ©æ‰‹ã€‚
è¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ã€‚
å›ç­”è¦å‡†ç¡®ã€ç®€æ´ï¼Œå¹¶æ ‡æ³¨ä¿¡æ¯æ¥æºã€‚
"""


class LlamaIndexQueryEngine:
    """LlamaIndex æŸ¥è¯¢å¼•æ“å°è£…"""

    def __init__(
        self,
        index: VectorStoreIndex,
        top_k: int = 3,
        similarity_cutoff: float = 0.5,
    ):
        self.index = index
        self.top_k = top_k

        # é…ç½®æ£€ç´¢å™¨
        retriever = VectorIndexRetriever(
            index=index,
            similarity_top_k=top_k,
        )

        # é…ç½®åå¤„ç†å™¨
        postprocessor = SimilarityPostprocessor(similarity_cutoff=similarity_cutoff)

        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        self.query_engine = RetrieverQueryEngine.from_args(
            retriever=retriever,
            node_postprocessors=[postprocessor],
        )

    def query(self, question: str) -> LlamaIndexResponse:
        """æ‰§è¡ŒæŸ¥è¯¢"""
        response = self.query_engine.query(question)

        sources = []
        for node in response.source_nodes:
            sources.append({
                "content": node.text[:200] + "..." if len(node.text) > 200 else node.text,
                "score": node.score or 0.0,
                "metadata": node.metadata,
            })

        return LlamaIndexResponse(
            answer=str(response),
            sources=sources,
        )
```

### Step 8: æ·»åŠ  LlamaIndex API

æ›´æ–° `src/chatbi/main.py`ï¼š

```python
from chatbi.rag_llamaindex.index import load_llamaindex
from chatbi.rag_llamaindex.query import LlamaIndexQueryEngine

# LlamaIndex å¼•æ“ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
_llamaindex_engine: LlamaIndexQueryEngine | None = None


def get_llamaindex_engine() -> LlamaIndexQueryEngine:
    global _llamaindex_engine
    if _llamaindex_engine is None:
        index = load_llamaindex()
        _llamaindex_engine = LlamaIndexQueryEngine(index)
    return _llamaindex_engine


@app.post("/ask_rag", response_model=AskRAGResponse)
async def ask_rag(request: AskRAGRequest):
    """LlamaIndex RAG æŸ¥è¯¢æ¥å£"""
    engine = get_llamaindex_engine()
    result = engine.query(request.question)

    sources = [
        SourceItem(
            content=s["content"],
            score=s["score"],
            source=s["metadata"].get("file_name", "æœªçŸ¥"),
        )
        for s in result.sources
    ]

    return AskRAGResponse(answer=result.answer, sources=sources)
```

### Step 9: æµ‹è¯• LlamaIndex RAG

```bash
# 1. æ„å»º LlamaIndex ç´¢å¼•
python -c "
from chatbi.rag_llamaindex.index import build_llamaindex
build_llamaindex('./data/docs')
"

# 2. æµ‹è¯•æŸ¥è¯¢
curl -X POST http://localhost:8000/ask_rag \
  -H "Content-Type: application/json" \
  -d '{"question": "ä»€ä¹ˆæ˜¯ DAUï¼Ÿå¦‚ä½•è®¡ç®—ï¼Ÿ"}'
```

---

## éªŒæ”¶æ£€æŸ¥æ¸…å•

### Part 1: æ‰‹å†™ RAG
- [ ] æ–‡æ¡£åŠ è½½å™¨å¯ä»¥è¯»å– Markdown/æ–‡æœ¬æ–‡ä»¶
- [ ] æ–‡æœ¬åˆ‡åˆ†å™¨èƒ½æŒ‰ token é™åˆ¶åˆ‡å—
- [ ] Embedding å‘é‡åŒ–æ­£å¸¸å·¥ä½œ
- [ ] Chroma å‘é‡å­˜å‚¨å¯ä»¥æ·»åŠ å’Œæ£€ç´¢
- [ ] `POST /ask_rag_raw` æ¥å£è¿”å›æ­£ç¡®ç­”æ¡ˆå’Œæ¥æº

### Part 2: LlamaIndex RAG
- [ ] LlamaIndex ç´¢å¼•æ„å»ºæˆåŠŸ
- [ ] ç´¢å¼•å¯ä»¥æŒä¹…åŒ–å’ŒåŠ è½½
- [ ] `POST /ask_rag` æ¥å£æ­£å¸¸å·¥ä½œ
- [ ] æ£€ç´¢ç»“æœåŒ…å«ç›¸å…³æ€§åˆ†æ•°å’Œæ¥æºä¿¡æ¯

---

## ä¸‹ä¸€æ­¥

å®Œæˆæœ¬é˜¶æ®µåï¼Œè¿›å…¥ [é˜¶æ®µ 3ï¼šæœåŠ¡åŒ–ã€Web Demoã€Text-to-SQL](phase-3-service-webdemo-text2sql.md)
